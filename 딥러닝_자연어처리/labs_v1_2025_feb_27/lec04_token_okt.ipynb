{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af3b75d1-ece8-4286-959e-c00b0a059873",
   "metadata": {},
   "source": [
    "## 2. Keras를 이용한 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc59b5fd-bd2a-4c17-a611-fac602261649",
   "metadata": {},
   "source": [
    "토큰화 함수를 이용하자. NLTK 토큰화 모듈과는 약간 다르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea5355ff-f069-47bd-aba3-448ef2c6a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75cc3641-e41f-42a1-8dea-095378a3a261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어집합 :  {'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "text = ['I love my dog', 'I, love my cat', 'You love my dog!']\n",
    "\n",
    "t  = Tokenizer()\n",
    "t.fit_on_texts(text)\n",
    "print(\"단어집합 : \", t.word_index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5961c625-a105-472e-8d45-5c0192de3e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: ['I love my dog', 'I, love my cat', 'You love my dog!']\n",
      "seq : [[3, 1, 2, 4], [3, 1, 2, 5], [6, 1, 2, 4]]\n"
     ]
    }
   ],
   "source": [
    "seq = t.texts_to_sequences(text)\n",
    "print('text:', text) \n",
    "print('seq :', seq) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a229a-fd32-4477-b204-3b0b63bec7a4",
   "metadata": {},
   "source": [
    "원-핫 인코딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3358cf0-3cd2-4678-aa74-86907e34d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb0df23c-55b9-4983-8e2f-3fd7eaf427d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text= ['cat', 'dog', 'cat', 'bird']\n"
     ]
    }
   ],
   "source": [
    "text = [\"cat\", \"dog\", \"cat\", \"bird\"]  # 우리가 변환하고 싶은 텍스트\n",
    "total_pets = [\"cat\", \"dog\", \"turtle\", \"fish\", \"bird\"]  # 단어 집합\n",
    "print(\"text=\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27ef281f-f7e9-42e3-b90d-7679ffec03a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 0, 'dog': 1, 'turtle': 2, 'fish': 3, 'bird': 4}\n"
     ]
    }
   ],
   "source": [
    "mapping = {}     # 변환에 사용되는 딕셔너리를 만든다. \n",
    "for x in range(len(total_pets)):\n",
    "  mapping[total_pets[x]] = x\t#“cat\"->0, \"dog\"->1, ...\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "414cecad-ff49-4312-9fc2-2a1025fd6169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text= [0, 1, 0, 4]\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(text)):    # 단어들을 순차적인 정수인덱스fh \n",
    "    text[x] = mapping[text[x]]\n",
    "print(\"text=\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "303312cb-2fca-4306-ada1-2f9d2106f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text= [[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "one_hot_encode = to_categorical(text) # 정수인덱스를 원-핫 인코딩 \n",
    "print(\"text=\", one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd896b6c-8336-4ddb-b094-7cd555db97e5",
   "metadata": {},
   "source": [
    "패딩(padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450ba596-77ba-4fcc-a0be-949ef1df2d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f104fab0-5bbd-4cfd-8572-7e1fc3b7b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = ['I love my dog', 'I, love my cat', 'You love my dog!']\n",
    "text = ['토큰 이해함. ㅎㅎ', \n",
    "        '정수 인코딩이 원-핫 인코딩인가? 가물 가물! ㅋㅋ', \n",
    "        '이제 슬슬 졸린다. 자연어 처리에서 패딩을 배우고 있는데 이게 뭐지?']\n",
    "tokenizer  = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "word_inx = tokenizer.word_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9dad7e-8150-4f0c-b370-4efeee560273",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩:\n",
      "\n",
      " [[2, 3, 4], [5, 6, 7, 8, 9, 1, 1, 10], [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]]\n",
      "\n",
      "pre 패딩:\n",
      "\n",
      " [[ 0  0  0  0  0  0  0  0  0  0  0  0  2  3  4]\n",
      " [ 0  0  0  0  0  0  0  5  6  7  8  9  1  1 10]\n",
      " [ 0  0  0  0  0 11 12 13 14 15 16 17 18 19 20]]\n",
      "\n",
      "post 패딩:\n",
      "\n",
      " [[ 2  3  4  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 5  6  7  8  9  1  1 10  0  0  0  0  0  0  0]\n",
      " [11 12 13 14 15 16 17 18 19 20  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "int_enc = tokenizer.texts_to_sequences(text) \n",
    "print('정수 인코딩:\\n\\n', int_enc)\n",
    "max_length = 15\n",
    "pad_int_enc = pad_sequences(int_enc, maxlen=max_length, padding='pre')\n",
    "print(\"\\npre 패딩:\\n\\n\",pad_int_enc)\n",
    "pad_int_enc = pad_sequences(int_enc, maxlen=max_length, padding='post')\n",
    "print(\"\\npost 패딩:\\n\\n\", pad_int_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db07fe3b-5465-4762-9fa1-5ba9e39aebdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0026fbdd-3409-4141-8a37-3a5cbe913810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
